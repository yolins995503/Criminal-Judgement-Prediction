# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PzWsv80KNHIagowMx7rGQJfoEZ_Ll5Q1
"""

#import packages
import re
import csv
import keras
import time
import numpy as np
import pandas as pd
import random as rd
import matplotlib.pyplot as plt
#word2vec
from gensim.models import Word2Vec
from gensim import models
import jieba

# tensorflow-keras
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, Bidirectional
from keras.utils import to_categorical
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.optimizers import RMSprop
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau

#進行訓練和測試樣本的分割
from sklearn.model_selection import train_test_split

import warnings
warnings.filterwarnings("ignore")

#讀入word2vec model
word2vec_model = models.Word2Vec.load('all_class.model')
print('Found %s word vectors of word2vec' % len(word2vec_model.wv.vocab))

#讀入Data
data = pd.read_csv(r'judgement_predict_test_data.csv',encoding='utf-8')

cols = ['事實及理由','label']
train = data.loc[:, cols]
train['事實及理由'] = pd.DataFrame(train['事實及理由'].astype(str))

#顯示前十筆Data和正負樣本數
print(train.label.value_counts())
data.head(1500)
#data.tail(5)

train_texts_orig = [] #用來儲存所有事實
for i in train['事實及理由'] :
    train_texts_orig.append(i)
print(len(train_texts_orig))

# 分詞和tokenize
seconds = time.time()
# 將秒數轉為本地時間
local_time = time.ctime(seconds)
# 輸出結果
print("本地時間：", local_time)

train_tokens = []
count = 0
for text in train_texts_orig:
    # 去掉标点
    text = re.sub(r"[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+", "",text)
    # 结巴分词
    cut = jieba.cut(text)
    # 结巴分词的输出结果为一个生成器
    # 把生成器转换为list
    cut_list = [ i for i in cut ]
    for i, word in enumerate(cut_list):
        try:
            # 将词转换为索引index
            cut_list[i] = word2vec_model.wv.vocab[word].index
        except KeyError:
            # 如果词不在字典中，则输出0
            cut_list[i] = 0
    train_tokens.append(cut_list)
    count+=1
    if(count % 100 == 0):
        print('已處理文章數 : ' + str(count))
print('文章分段完成')

seconds = time.time()
# 將秒數轉為本地時間
local_time = time.ctime(seconds)
# 輸出結果
print("本地時間：", local_time)

# 獲得所有tokens的長度
num_tokens = [ len(tokens) for tokens in train_tokens ]
num_tokens = np.array(num_tokens)

# 平均tokens的長度
np.mean(num_tokens)

# 最長的tokens的長度
np.max(num_tokens)

plt.hist(np.log(num_tokens), bins = 100)
plt.xlim((0,10))
plt.ylabel('number of tokens')
plt.xlabel('length of tokens')
plt.title('Distribution of tokens length')
plt.show()

# 取tokens平均值并加上两个tokens的标准差，
# 假设tokens长度的分布为正态分布，则max_tokens这个值可以涵盖95%左右的样本
max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)
max_tokens = int(max_tokens)
print(max_tokens)

# 取tokens的长度为236时，大约95%的样本被涵盖
# 我们对长度不足的进行padding，超长的进行修剪
np.sum( num_tokens < max_tokens ) / len(num_tokens)

# 用来将tokens转换为文本
def reverse_tokens(tokens):
    text = ''
    for i in tokens:
        if i != 0:
            text = text + word2vec_model.wv.index2word[i]
        else:
            text = text + ' '
    return text
reverse = reverse_tokens(train_tokens[0])
reverse

embedding_dim = 250
num_words = len(word2vec_model.wv.vocab)
# 初始化embedding_matrix，之后在keras上进行应用
embedding_matrix = np.zeros((num_words, embedding_dim))
# embedding_matrix为一个 [num_words，embedding_dim] 的矩阵
# 维度为 20000 * 300
for i in range(num_words):
    embedding_matrix[i,:] = word2vec_model[word2vec_model.wv.index2word[i]]
embedding_matrix = embedding_matrix.astype('float32')

# embedding_matrix的维度，
# 这个维度为keras的要求，后续会在模型中用到
embedding_matrix.shape

np.sum( word2vec_model[word2vec_model.wv.index2word[250]] == embedding_matrix[250] )

# 进行padding和truncating， 输入的train_tokens是一个list
# 返回的train_pad是一个numpy array
train_pad = pad_sequences(train_tokens, maxlen=max_tokens,padding='pre', truncating='pre')

# 超出五万个词向量的词用0代替
train_pad[ train_pad>=num_words ] = 0

# 可见padding之后前面的tokens全变成0，文本在最后面
train_pad[33]

# 准备target向量，前2000样本为1，后2000为0
train_target = np.array(train['label'])
train_target = to_categorical(train_target)
print(train_target)

print(train_target.shape)

# 90%的样本用来训练，剩余10%用来测试
X_train, X_test, y_train, y_test = train_test_split(train_pad,
                                                    train_target,
                                                    test_size=0.1,
                                                    random_state=12)


# 查看训练样本，确认无误
print(reverse_tokens(X_train[8]))
print('class: ',y_train[8])

# 用LSTM对样本进行分类
model = Sequential()

# 模型第一层为embedding
model.add(Embedding(num_words,
                    embedding_dim,
                    weights=[embedding_matrix],
                    input_length=max_tokens,
                    trainable=False))

model.add(Bidirectional(LSTM(units=32, return_sequences=True)))
model.add(LSTM(units=16, return_sequences=False))


model.add(Dense(8, activation='softmax'))
# 我们使用adam以0.001的learning rate进行优化
optimizer = Adam(lr=1e-3)

model.compile(loss='categorical_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])

model.summary()


# 建立一个权重的存储点
path_checkpoint = 'judgement_checkpoint.keras'
checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',
                                      verbose=1, save_weights_only=True,
                                      save_best_only=True)

# 尝试加载已训练模型
try:
    model.load_weights(path_checkpoint)
except Exception as e:
    print(e)

# 定义early stoping如果3个epoch内validation loss没有改善则停止训练
earlystopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)


# 自动降低learning rate
lr_reduction = ReduceLROnPlateau(monitor='val_loss',
                                       factor=0.1, min_lr=1e-5, patience=0,
                                       verbose=1)

# 定义callback函数
callbacks = [
    earlystopping, 
    checkpoint,
    lr_reduction
]


# 开始训练
model.fit(X_train, y_train,
          validation_split=0.1, 
          epochs=20,
          batch_size=128,
          callbacks=callbacks)

result = model.evaluate(X_test, y_test)
print('Accuracy:{0:.2%}'.format(result[1]))

model.save('LSTM_model.model')
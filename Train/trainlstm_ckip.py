# -*- coding: utf-8 -*-
"""LSTM_judgement.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14TKlcpDoNt6IjR5FNyf61cEKmP0yI45F
"""
#%%
#import packages
import re
import csv
import keras
import time
import numpy as np
import pandas as pd
import random as rd
import matplotlib.pyplot as plt

#word2vec
from gensim.models import Word2Vec
from gensim import models

#斷詞
import jieba
from ckiptagger import data_utils, construct_dictionary, WS
# Load model
ws = WS(r'D:\pysource\ckiptagger\data')

# keras
from keras.models import Sequential
from keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional
from keras.utils import to_categorical
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.optimizers import RMSprop
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau

#進行訓練和測試樣本的分割
from sklearn.model_selection import train_test_split

import warnings
warnings.filterwarnings("ignore")
#%%
#讀入word2vec model
word2vec_model = models.Word2Vec.load('all_class.model')
print('Found %s word vectors of word2vec' % len(word2vec_model.wv.vocab))

#讀入Data
data = pd.read_csv('class_classification_test_train.csv',encoding='utf-8')

cols = ['事實及理由','label']
train = data.loc[:, cols]
train['事實及理由'] = pd.DataFrame(train['事實及理由'].astype(str))

#顯示前十筆Data和正負樣本數
print(train.label.value_counts())
data.head(400)

#%%
train_texts_orig = [] #用來儲存所有事實
for i in train['事實及理由'] :
    train_texts_orig.append(i)
print(len(train_texts_orig))
#%%
# 分詞和tokenize
seconds = time.time()
local_time = time.ctime(seconds)
print("本地時間：", local_time)

'''
train_tokens = []
count = 0
for text in train_texts_orig:
    text = re.sub(r"[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&*（）]+", "",text)
    cut = jieba.cut(text)
    cut_list = [ i for i in cut ]
    for i, word in enumerate(cut_list):
        try:
            cut_list[i] = word2vec_model.wv.vocab[word].index
        except KeyError:
            cut_list[i] = 0
    train_tokens.append(cut_list)
    count+=1
    if(count % 100 == 0):
        print('已處理文章數 : ' + str(count))
print('文章分段完成')
'''

train_tokens = []
count = 0
for text in train_texts_orig:
    text_list = []
    rule = re.compile(u"[^a-zA-Z\u4e00-\u9fa5]")
    text = rule.sub('',text)
    text_list.append(text)
    cut = ws(text_list)
    cut_list = [ i for i in cut ]
    for i, word in enumerate(cut_list[0]):
        try:
            cut_list[0][i] = word2vec_model.wv.vocab[word].index
        except KeyError:
            cut_list[0][i] = 0
    train_tokens.append(cut_list[0])
    count+=1
    if(count % 10 == 0):
        print('已處理文章數 : ' + str(count))
print('文章分段完成')

seconds = time.time()
local_time = time.ctime(seconds)
print("本地時間：", local_time)
'''
#%%
# 獲得所有tokens的長度
num_tokens = [ len(tokens) for tokens in train_tokens ]
num_tokens = np.array(num_tokens)

# 平均tokens的長度
np.mean(num_tokens)

# 最長的tokens的長度
np.max(num_tokens)


plt.hist(np.log(num_tokens), bins = 100)
plt.xlim((0,10))
plt.ylabel('number of tokens')
plt.xlabel('length of tokens')
plt.title('Distribution of tokens length')
#plt.show()


# 取tokens平均值並加上兩个tokens的標準差，
# 假設tokens長度的分布為正態分布，則max_tokens這個值可以涵盖95%左右的樣本

max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)
max_tokens = int(max_tokens)
print(max_tokens)
'''
max_tokens = 980
#%%
# 用来將tokens轉換成文本
def reverse_tokens(tokens):
    text = ''
    for i in tokens:
        if i != 0:
            text = text + word2vec_model.wv.index2word[i]
        else:
            text = text + ' '
    return text
reverse = reverse_tokens(train_tokens[0])
reverse
#%%
embedding_dim = 250
num_words = len(word2vec_model.wv.vocab)

# 初始化 embedding_matrix (size 為 embedding_dim*num_words)
embedding_matrix = np.zeros((num_words, embedding_dim))
for i in range(num_words):
    embedding_matrix[i,:] = word2vec_model[word2vec_model.wv.index2word[i]]
embedding_matrix = embedding_matrix.astype('float32')

# embedding_matrix的維度
embedding_matrix.shape

np.sum( word2vec_model[word2vec_model.wv.index2word[250]] == embedding_matrix[250] )

# 進行padding和truncating， 輸入的train_tokens是一个list
# 返回的train_pad是一个numpy array
train_pad = pad_sequences(train_tokens, maxlen=max_tokens,padding='pre', truncating='pre')

# 超出五萬個詞向量的詞用0代替
train_pad[ train_pad>=num_words ] = 0

# 可见padding之后前面的tokens全变成0，文本在最后面
train_pad[33]

# 準備target向量(用以標記label)
train_target = np.array(train['label'])
train_target = to_categorical(train_target)
print(train_target)

print(train_target.shape)

# 90%的樣本用來訓練，剩餘10%用來測試
X_train, X_test, y_train, y_test = train_test_split(train_pad,
                                                    train_target,
                                                    test_size=0.1,
                                                    random_state=12)


# 查看訓練樣本
print(reverse_tokens(X_train[8]))
print('class: ',y_train[8])

#%%
# 用LSTM对样本进行分类
model = Sequential()

# 模型第一层为embedding
model.add(Embedding(num_words,
                    embedding_dim,
                    weights=[embedding_matrix],
                    input_length=max_tokens,
                    trainable=False))

model.add(Bidirectional(LSTM(units=32, return_sequences=True)))
model.add(LSTM(units=16, return_sequences=False))


model.add(Dense(6, activation='softmax'))
# 我们使用adam以0.001的learning rate进行优化
optimizer = Adam(lr=1e-3)

model.compile(loss='categorical_crossentropy',
              optimizer=optimizer,
              metrics=['accuracy'])

model.summary()


# 建立一个权重的存储点
path_checkpoint = 'judgement_checkpoint.keras'
checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',
                                      verbose=1, save_weights_only=True,
                                      save_best_only=True)

# 尝试加载已训练模型
try:
    model.load_weights(path_checkpoint)
except Exception as e:
    print(e)

# 定义early stoping如果3个epoch内validation loss没有改善则停止训练
earlystopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)


# 自动降低learning rate
lr_reduction = ReduceLROnPlateau(monitor='val_loss',
                                       factor=0.1, min_lr=1e-5, patience=0,
                                       verbose=1)

# 定义callback函数
callbacks = [
    earlystopping, 
    checkpoint,
    lr_reduction
]


# 开始训练
model.fit(X_train, y_train,
          validation_split=0.1, 
          epochs=20,
          batch_size=128,
          callbacks=callbacks)

result = model.evaluate(X_test, y_test)
print('Accuracy:{0:.2%}'.format(result[1]))

#%%
#儲存model
model.save('LSTM_class.model')